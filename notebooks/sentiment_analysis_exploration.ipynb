{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54831f3",
   "metadata": {},
   "source": [
    "# Customer Review Sentiment Analysis\n",
    "\n",
    "This notebook provides an interactive exploration of customer review sentiment analysis using various deep learning models.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify customer reviews as positive or negative\n",
    "- **Models**: Neural Bag of Words (NBoW), LSTM, CNN, Transformer (BERT)\n",
    "- **Framework**: PyTorch with modern NLP techniques\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Data Exploration](#exploration)\n",
    "3. [Text Preprocessing](#preprocessing)\n",
    "4. [Model Training](#training)\n",
    "5. [Model Evaluation](#evaluation)\n",
    "6. [Inference and Prediction](#inference)\n",
    "7. [Model Comparison](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309cd19",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading {#setup}\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c9f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cpu\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Please run: pip install -r requirements.txt\")\n",
    "\n",
    "# Import our modules\n",
    "from utils.data_loader import ReviewDataset, CustomDataLoader\n",
    "from utils.preprocessing import TextPreprocessor, VocabularyBuilder\n",
    "from utils.training import Trainer\n",
    "from utils.visualization import *\n",
    "from models.nbow import NBoW\n",
    "from models.lstm import LSTMModel\n",
    "from models.cnn import CNNModel\n",
    "from models.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835a9bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully: 50 reviews\n",
      "Columns: ['review_text', 'sentiment']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product is amazing! I love it so much.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible quality, waste of money.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great value for money, highly recommend.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poor customer service, disappointed.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excellent product, exceeded my expectations.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    review_text sentiment\n",
       "0   This product is amazing! I love it so much.  positive\n",
       "1             Terrible quality, waste of money.  negative\n",
       "2      Great value for money, highly recommend.  positive\n",
       "3          Poor customer service, disappointed.  negative\n",
       "4  Excellent product, exceeded my expectations.  positive"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load sample data\n",
    "data_path = '../data/sample_reviews.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Data loaded successfully: {len(df)} reviews\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Data file not found at {data_path}\")\n",
    "    print(\"Creating sample data...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_reviews = [\n",
    "        (\"This product is amazing! I love it so much.\", \"positive\"),\n",
    "        (\"Terrible quality, waste of money.\", \"negative\"),\n",
    "        (\"Great value for money, highly recommend.\", \"positive\"),\n",
    "        (\"Poor customer service, disappointed.\", \"negative\"),\n",
    "        (\"Excellent product, exceeded my expectations.\", \"positive\"),\n",
    "        (\"Not worth the price, very disappointed.\", \"negative\"),\n",
    "        (\"Perfect for my needs, very satisfied.\", \"positive\"),\n",
    "        (\"Broke after one day, terrible quality.\", \"negative\"),\n",
    "        (\"Outstanding quality and fast delivery.\", \"positive\"),\n",
    "        (\"Completely useless, don't buy this.\", \"negative\")\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(sample_reviews, columns=['review_text', 'sentiment'])\n",
    "    print(\"Sample data created:\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4e4d2",
   "metadata": {},
   "source": [
    "## 2. Data Exploration {#exploration}\n",
    "\n",
    "Let's explore our dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Data types: {df.dtypes.to_dict()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().to_dict()}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "print(f\"Positive ratio: {sentiment_counts.get('positive', 0) / len(df):.2%}\")\n",
    "print(f\"Negative ratio: {sentiment_counts.get('negative', 0) / len(df):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sentiment distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "text_lengths = df['review_text'].str.len()\n",
    "plt.hist(text_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(text_lengths.mean(), color='red', linestyle='--', label=f'Mean: {text_lengths.mean():.1f}')\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Review Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count analysis\n",
    "df['word_count'] = df['review_text'].str.split().str.len()\n",
    "\n",
    "print(\"Word Count Statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Word count by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    subset = df[df['sentiment'] == sentiment]\n",
    "    plt.hist(subset['word_count'], alpha=0.7, label=sentiment, bins=10)\n",
    "\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count Distribution by Sentiment')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3f2a6",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing {#preprocessing}\n",
    "\n",
    "Now let's preprocess the text data for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a278a847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: en_core_web_sm model not found. Using basic tokenization.\n",
      "Text Preprocessing Example:\n",
      "Original: This product is AMAZING!!! I love it so much. Best purchase ever! üòç\n",
      "Cleaned: this product is amazing!!! i love it so much. best purchase ever! üòç\n",
      "Tokenized: ['This', 'product', 'is', 'AMAZING!!!', 'I', 'love', 'it', 'so', 'much.', 'Best', 'purchase', 'ever!', 'üòç']\n",
      "Preprocessed: ['product', 'amazing!!!', 'love', 'so', 'much.', 'best', 'purchase', 'ever!']\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Example preprocessing\n",
    "sample_text = \"This product is AMAZING!!! I love it so much. Best purchase ever! üòç\"\n",
    "\n",
    "print(\"Text Preprocessing Example:\")\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {preprocessor.clean_text(sample_text)}\")\n",
    "print(f\"Tokenized: {preprocessor.tokenize(sample_text)}\")\n",
    "print(f\"Preprocessed: {preprocessor.preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e46274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing all reviews...\n",
      "\n",
      "Processing Examples:\n",
      "Original: This product is amazing! I love it so much.\n",
      "Processed: ['product', 'amazing!', 'love', 'so', 'much.']\n",
      "--------------------------------------------------\n",
      "Original: Terrible quality, waste of money.\n",
      "Processed: ['terrible', 'quality,', 'waste', 'money.']\n",
      "--------------------------------------------------\n",
      "Original: Great value for money, highly recommend.\n",
      "Processed: ['great', 'value', 'money,', 'highly', 'recommend.']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Preprocess all reviews\n",
    "print(\"Preprocessing all reviews...\")\n",
    "texts = df['review_text'].astype(str).tolist()\n",
    "processed_texts = [preprocessor.preprocess_text(text) for text in texts]\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nProcessing Examples:\")\n",
    "for i in range(min(3, len(texts))):\n",
    "    print(f\"Original: {texts[i]}\")\n",
    "    print(f\"Processed: {processed_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec65d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 104\n",
      "Sample vocabulary: [('<pad>', 0), ('<unk>', 1), ('product,', 2), ('great', 3), ('not', 4), ('poor', 5), ('quality.', 6), ('very', 7), ('terrible', 8), ('quality,', 9)]\n",
      "\n",
      "Text to indices example:\n",
      "Tokens: ['product', 'amazing!', 'love', 'so', 'much.']\n",
      "Indices: [10, 51, 17, 52, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab_builder = VocabularyBuilder(min_freq=1, max_vocab_size=1000)\n",
    "vocab = vocab_builder.build_from_texts(processed_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample vocabulary: {list(vocab.items())[:10]}\")\n",
    "\n",
    "# Convert text to indices\n",
    "max_length = 50\n",
    "text_indices = [vocab_builder.text_to_indices(tokens, max_length) for tokens in processed_texts]\n",
    "\n",
    "print(f\"\\nText to indices example:\")\n",
    "print(f\"Tokens: {processed_texts[0]}\")\n",
    "print(f\"Indices: {text_indices[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b04d5",
   "metadata": {},
   "source": [
    "## 4. Model Training {#training}\n",
    "\n",
    "Let's train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099ae34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40\n",
      "Test samples: 10\n",
      "Vocabulary size: 104\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to integers\n",
    "labels = df['sentiment'].tolist()\n",
    "label_to_idx = {'negative': 0, 'positive': 1}\n",
    "y = [label_to_idx[label] for label in labels]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_indices, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create simple dataset class for tokenized data\n",
    "class TokenizedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TokenizedDataset(X_train, y_train)\n",
    "test_dataset = TokenizedDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc4870b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Neural Bag of Words (NBoW) model...\n",
      "Starting training on cpu...\n",
      "Model parameters: 12,754\n",
      "Batch 0/5, Loss: 0.7309\n",
      "Epoch 1/10 (0.03s)\n",
      "Train - Loss: 0.7124, Acc: 0.4500\n",
      "Val - Loss: 0.6932, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.6813\n",
      "Epoch 2/10 (0.01s)\n",
      "Train - Loss: 0.7011, Acc: 0.4750\n",
      "Val - Loss: 0.6932, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.7224\n",
      "Epoch 3/10 (0.01s)\n",
      "Train - Loss: 0.6793, Acc: 0.5750\n",
      "Val - Loss: 0.6933, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.6938\n",
      "Epoch 4/10 (0.01s)\n",
      "Train - Loss: 0.7040, Acc: 0.4500\n",
      "Val - Loss: 0.6934, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.7029\n",
      "Epoch 5/10 (0.01s)\n",
      "Train - Loss: 0.7162, Acc: 0.4250\n",
      "Val - Loss: 0.6931, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.7535\n",
      "Epoch 6/10 (0.01s)\n",
      "Train - Loss: 0.6962, Acc: 0.5250\n",
      "Val - Loss: 0.6927, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.6772\n",
      "Epoch 7/10 (0.01s)\n",
      "Train - Loss: 0.7109, Acc: 0.5750\n",
      "Val - Loss: 0.6925, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.6698\n",
      "Epoch 8/10 (0.01s)\n",
      "Train - Loss: 0.6796, Acc: 0.6000\n",
      "Val - Loss: 0.6925, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.7025\n",
      "Epoch 9/10 (0.01s)\n",
      "Train - Loss: 0.6972, Acc: 0.4500\n",
      "Val - Loss: 0.6925, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "Batch 0/5, Loss: 0.6931\n",
      "Epoch 10/10 (0.01s)\n",
      "Train - Loss: 0.6962, Acc: 0.5500\n",
      "Val - Loss: 0.6925, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------\n",
      "Loaded best model with validation loss: 0.6925\n",
      "NBoW training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train Neural Bag of Words model\n",
    "print(\"Training Neural Bag of Words (NBoW) model...\")\n",
    "\n",
    "# Create model\n",
    "nbow_model = NBoW(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=50,\n",
    "    hidden_dim=64,\n",
    "    output_dim=2,\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(nbow_model, device=device)\n",
    "\n",
    "# Train model\n",
    "history = trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,  # Using test as validation for demo\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"NBoW training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0520773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating NBoW model...\")\n",
    "results = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {results['precision']:.4f}\")\n",
    "print(f\"Test Recall: {results['recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {results['f1']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = ['Negative', 'Positive']\n",
    "plot_confusion_matrix(results['confusion_matrix'], class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9dbc3c",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation {#evaluation}\n",
    "\n",
    "Let's evaluate our model's performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(results['labels'], results['predictions'], \n",
    "                          target_names=class_names))\n",
    "\n",
    "# Plot classification report\n",
    "plot_classification_report(results['labels'], results['predictions'], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "test_df = pd.DataFrame({\n",
    "    'review': [texts[i] for i in range(len(X_test))],\n",
    "    'true_label': [class_names[label] for label in results['labels']],\n",
    "    'predicted_label': [class_names[pred] for pred in results['predictions']],\n",
    "    'confidence': [max(prob) for prob in results['probabilities']]\n",
    "})\n",
    "\n",
    "# Show correct predictions\n",
    "correct_predictions = test_df[test_df['true_label'] == test_df['predicted_label']]\n",
    "print(f\"Correct predictions: {len(correct_predictions)}\")\n",
    "print(\"\\nSample correct predictions:\")\n",
    "display(correct_predictions.head())\n",
    "\n",
    "# Show incorrect predictions\n",
    "incorrect_predictions = test_df[test_df['true_label'] != test_df['predicted_label']]\n",
    "print(f\"\\nIncorrect predictions: {len(incorrect_predictions)}\")\n",
    "if len(incorrect_predictions) > 0:\n",
    "    print(\"Sample incorrect predictions:\")\n",
    "    display(incorrect_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770867cf",
   "metadata": {},
   "source": [
    "## 6. Inference and Prediction {#inference}\n",
    "\n",
    "Let's test our model on new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54543102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab_builder, preprocessor, device):\n",
    "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess text\n",
    "    tokens = preprocessor.preprocess_text(text)\n",
    "    indices = vocab_builder.text_to_indices(tokens, max_length)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': class_names[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'negative': probabilities[0][0].item(),\n",
    "            'positive': probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on new reviews\n",
    "test_reviews = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"Terrible quality, complete waste of money.\",\n",
    "    \"Pretty good, but could be better.\",\n",
    "    \"Absolutely love this! Highly recommend.\",\n",
    "    \"Not bad, but not great either.\"\n",
    "]\n",
    "\n",
    "print(\"Testing on new reviews:\")\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, nbow_model, vocab_builder, preprocessor, device)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"Probabilities: {result['probabilities']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction\n",
    "def interactive_prediction():\n",
    "    \"\"\"Interactive sentiment prediction.\"\"\"\n",
    "    print(\"Interactive Sentiment Analysis\")\n",
    "    print(\"Enter a review to analyze (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour review: \")\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if user_input.strip():\n",
    "            result = predict_sentiment(user_input, nbow_model, vocab_builder, preprocessor, device)\n",
    "            print(f\"Sentiment: {result['sentiment']}\")\n",
    "            print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"Probabilities: {result['probabilities']}\")\n",
    "        else:\n",
    "            print(\"Please enter a valid review.\")\n",
    "\n",
    "# Uncomment to run interactive prediction\n",
    "# interactive_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2af80e",
   "metadata": {},
   "source": [
    "## 7. Model Comparison {#comparison}\n",
    "\n",
    "Let's compare different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, model_name, train_loader, test_loader, epochs=10):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    trainer = Trainer(model, device=device)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        epochs=epochs,\n",
    "        lr=1e-3,\n",
    "        early_stopping_patience=5\n",
    "    )\n",
    "    \n",
    "    results = trainer.evaluate(test_loader)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'trainer': trainer,\n",
    "        'history': history,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# Compare models\n",
    "model_results = {}\n",
    "\n",
    "# NBoW (already trained)\n",
    "model_results['NBoW'] = {\n",
    "    'accuracy': results['accuracy'],\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1']\n",
    "}\n",
    "\n",
    "# LSTM Model\n",
    "try:\n",
    "    lstm_model = LSTMModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=50,\n",
    "        hidden_dim=64,\n",
    "        output_dim=2,\n",
    "        num_layers=2,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    lstm_result = train_and_evaluate_model(lstm_model, 'LSTM', train_loader, test_loader, epochs=5)\n",
    "    model_results['LSTM'] = {\n",
    "        'accuracy': lstm_result['results']['accuracy'],\n",
    "        'precision': lstm_result['results']['precision'],\n",
    "        'recall': lstm_result['results']['recall'],\n",
    "        'f1': lstm_result['results']['f1']\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"Error training LSTM: {e}\")\n",
    "\n",
    "# CNN Model\n",
    "try:\n",
    "    cnn_model = CNNModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=50,\n",
    "        output_dim=2,\n",
    "        filter_sizes=[3, 4, 5],\n",
    "        num_filters=50,\n",
    "        dropout_rate=0.5\n",
    "    )\n",
    "    \n",
    "    cnn_result = train_and_evaluate_model(cnn_model, 'CNN', train_loader, test_loader, epochs=5)\n",
    "    model_results['CNN'] = {\n",
    "        'accuracy': cnn_result['results']['accuracy'],\n",
    "        'precision': cnn_result['results']['precision'],\n",
    "        'recall': cnn_result['results']['recall'],\n",
    "        'f1': cnn_result['results']['f1']\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"Error training CNN: {e}\")\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad750f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "if len(model_results) > 1:\n",
    "    plot_model_comparison(model_results)\n",
    "else:\n",
    "    print(\"Need at least 2 models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dadac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading and Exploration**: Understanding the dataset structure and characteristics\n",
    "2. **Text Preprocessing**: Cleaning and preparing text data for neural networks\n",
    "3. **Model Training**: Training different deep learning models for sentiment analysis\n",
    "4. **Evaluation**: Comprehensive evaluation of model performance\n",
    "5. **Inference**: Making predictions on new data\n",
    "6. **Model Comparison**: Comparing different architectures\n",
    "\n",
    "### Key Takeaways:\n",
    "- Text preprocessing is crucial for good model performance\n",
    "- Different model architectures have different strengths\n",
    "- Proper evaluation metrics help understand model behavior\n",
    "- Interactive prediction enables practical use cases\n",
    "\n",
    "### Next Steps:\n",
    "1. Try the transformer model with pre-trained embeddings\n",
    "2. Experiment with different hyperparameters\n",
    "3. Use larger datasets for better generalization\n",
    "4. Implement more sophisticated preprocessing techniques\n",
    "5. Deploy the model for real-world applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e930b1d",
   "metadata": {},
   "source": [
    "## Model Improvement Strategy\n",
    "\n",
    "The current model has low accuracy due to several factors:\n",
    "1. **Very small dataset** (only 50 samples)\n",
    "2. **Poor text preprocessing** \n",
    "3. **Simple model architecture**\n",
    "4. **Limited regularization**\n",
    "\n",
    "Let's implement improvements to boost performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d572141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating expanded dataset...\n",
      "Expanded dataset size: 450\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "negative    225\n",
      "positive    225\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample reviews:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Amazing appliance, impressed my expectations.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Disappointing appliance, never worst.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Amazing solution, appreciate my expectations.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Disappointing product, extremely broken.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Perfect for my needs, totally excellent.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>This product is excellent! I enjoy it definitely.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Excellent equipment, love it truly.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Disappointing accessory, completely disappoint...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Perfect fit, exactly as advertised.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Defective tool, stopped immediately.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_text sentiment\n",
       "343      Amazing appliance, impressed my expectations.  positive\n",
       "330              Disappointing appliance, never worst.  negative\n",
       "431      Amazing solution, appreciate my expectations.  positive\n",
       "215           Disappointing product, extremely broken.  negative\n",
       "109           Perfect for my needs, totally excellent.  positive\n",
       "293  This product is excellent! I enjoy it definitely.  positive\n",
       "414                Excellent equipment, love it truly.  positive\n",
       "125  Disappointing accessory, completely disappoint...  negative\n",
       "179                Perfect fit, exactly as advertised.  positive\n",
       "250               Defective tool, stopped immediately.  negative"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a larger synthetic dataset for better training\n",
    "def create_expanded_dataset():\n",
    "    \"\"\"Create a larger, more diverse dataset.\"\"\"\n",
    "    \n",
    "    # Positive sentiment templates\n",
    "    positive_templates = [\n",
    "        \"This product is {adj}! I {verb} it {adv}.\",\n",
    "        \"{adj} quality, {adv} recommend it.\",\n",
    "        \"Amazing {noun}, {verb} my expectations.\",\n",
    "        \"Great {noun} for the price, very {adj}.\",\n",
    "        \"Outstanding {noun}, {adv} satisfied.\",\n",
    "        \"Perfect for my needs, {adv} {adj}.\",\n",
    "        \"Excellent {noun}, {verb} it {adv}.\",\n",
    "        \"Best {noun} I've {verb}, highly {adj}.\",\n",
    "        \"Superior quality, {adv} {adj}.\",\n",
    "        \"Fantastic {noun}, {verb} using it.\"\n",
    "    ]\n",
    "    \n",
    "    # Negative sentiment templates\n",
    "    negative_templates = [\n",
    "        \"This product is {adj}! I {verb} it.\",\n",
    "        \"Terrible {noun}, {adj} quality.\",\n",
    "        \"Poor {noun}, {adv} disappointed.\",\n",
    "        \"Waste of money, {adv} {adj}.\",\n",
    "        \"Horrible {noun}, {verb} after one day.\",\n",
    "        \"Cheap {noun}, not worth the price.\",\n",
    "        \"Disappointing {noun}, {adv} {adj}.\",\n",
    "        \"Worst {noun} ever, {adv} recommend.\",\n",
    "        \"Defective {noun}, {verb} immediately.\",\n",
    "        \"Useless {noun}, {adj} quality.\"\n",
    "    ]\n",
    "    \n",
    "    # Word pools for templates\n",
    "    positive_words = {\n",
    "        'adj': ['amazing', 'excellent', 'fantastic', 'wonderful', 'great', 'perfect', 'outstanding', 'superb', 'brilliant', 'awesome'],\n",
    "        'verb': ['love', 'adore', 'enjoy', 'appreciate', 'treasure', 'value', 'cherish', 'exceeded', 'surpassed', 'impressed'],\n",
    "        'adv': ['highly', 'strongly', 'definitely', 'absolutely', 'completely', 'totally', 'really', 'truly', 'very', 'extremely'],\n",
    "        'noun': ['product', 'item', 'purchase', 'device', 'tool', 'equipment', 'gadget', 'appliance', 'accessory', 'solution']\n",
    "    }\n",
    "    \n",
    "    negative_words = {\n",
    "        'adj': ['terrible', 'awful', 'horrible', 'disappointing', 'poor', 'bad', 'worst', 'useless', 'defective', 'broken'],\n",
    "        'verb': ['hate', 'dislike', 'regret', 'broke', 'failed', 'stopped', 'malfunctioned', 'disappointed', 'wasted', 'ruined'],\n",
    "        'adv': ['completely', 'totally', 'extremely', 'very', 'really', 'absolutely', 'utterly', 'highly', 'never', 'definitely'],\n",
    "        'noun': ['product', 'item', 'purchase', 'device', 'tool', 'equipment', 'gadget', 'appliance', 'accessory', 'waste']\n",
    "    }\n",
    "    \n",
    "    expanded_reviews = []\n",
    "    \n",
    "    # Generate positive reviews\n",
    "    for _ in range(200):\n",
    "        template = np.random.choice(positive_templates)\n",
    "        words = {key: np.random.choice(positive_words[key]) for key in positive_words}\n",
    "        review = template.format(**words)\n",
    "        expanded_reviews.append((review, 'positive'))\n",
    "    \n",
    "    # Generate negative reviews\n",
    "    for _ in range(200):\n",
    "        template = np.random.choice(negative_templates)\n",
    "        words = {key: np.random.choice(negative_words[key]) for key in negative_words}\n",
    "        review = template.format(**words)\n",
    "        expanded_reviews.append((review, 'negative'))\n",
    "    \n",
    "    # Add original reviews\n",
    "    for _, row in df.iterrows():\n",
    "        expanded_reviews.append((row['review_text'], row['sentiment']))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    expanded_df = pd.DataFrame(expanded_reviews, columns=['review_text', 'sentiment'])\n",
    "    expanded_df = expanded_df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "# Create expanded dataset\n",
    "print(\"Creating expanded dataset...\")\n",
    "expanded_df = create_expanded_dataset()\n",
    "print(f\"Expanded dataset size: {len(expanded_df)}\")\n",
    "print(f\"Sentiment distribution:\")\n",
    "print(expanded_df['sentiment'].value_counts())\n",
    "print(\"\\nSample reviews:\")\n",
    "display(expanded_df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c51cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Preprocessing Example:\n",
      "Original: This product is AMAZING!!! I love it so much. Best purchase ever! üòç\n",
      "Cleaned: this product is amazing i love it so much best purchase ever\n",
      "Tokenized: ['this', 'product', 'is', 'amazing', 'i', 'love', 'it', 'so', 'much', 'best', 'purchase', 'ever']\n",
      "Preprocessed: ['product', 'amazing', 'love', 'much', 'best', 'purchase', 'ever']\n",
      "\n",
      "Processing expanded dataset...\n",
      "Processed 450 reviews\n"
     ]
    }
   ],
   "source": [
    "# Improved text preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "class ImprovedTextPreprocessor:\n",
    "    \"\"\"Improved text preprocessor with better cleaning.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = {\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "            'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "            'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after',\n",
    "            'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "            'further', 'then', 'once'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return text.split()\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stop words from tokens.\"\"\"\n",
    "        return [token for token in tokens if token not in self.stop_words]\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        # Filter out very short tokens\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "        return tokens\n",
    "\n",
    "# Test improved preprocessor\n",
    "improved_preprocessor = ImprovedTextPreprocessor()\n",
    "\n",
    "sample_text = \"This product is AMAZING!!! I love it so much. Best purchase ever! üòç\"\n",
    "print(\"Improved Preprocessing Example:\")\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {improved_preprocessor.clean_text(sample_text)}\")\n",
    "print(f\"Tokenized: {improved_preprocessor.tokenize(improved_preprocessor.clean_text(sample_text))}\")\n",
    "print(f\"Preprocessed: {improved_preprocessor.preprocess_text(sample_text)}\")\n",
    "\n",
    "# Process expanded dataset\n",
    "print(\"\\nProcessing expanded dataset...\")\n",
    "expanded_texts = expanded_df['review_text'].astype(str).tolist()\n",
    "improved_processed_texts = [improved_preprocessor.preprocess_text(text) for text in expanded_texts]\n",
    "\n",
    "print(f\"Processed {len(improved_processed_texts)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4418ea87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved vocabulary size: 93\n",
      "Sample vocabulary: [('<pad>', 0), ('<unk>', 1), ('quality', 2), ('product', 3), ('very', 4), ('terrible', 5), ('price', 6), ('excellent', 7), ('purchase', 8), ('great', 9), ('amazing', 10), ('perfect', 11), ('waste', 12), ('defective', 13), ('highly', 14)]\n",
      "\n",
      "Improved text to indices example:\n",
      "Tokens: ['broke', 'one', 'day', 'terrible', 'quality']\n",
      "Indices: [64, 34, 31, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Improved dataset split:\n",
      "Training samples: 360\n",
      "Test samples: 90\n",
      "Vocabulary size: 93\n",
      "Batch size: 32\n",
      "Number of training batches: 12\n",
      "Number of test batches: 3\n"
     ]
    }
   ],
   "source": [
    "# Build improved vocabulary\n",
    "improved_vocab_builder = VocabularyBuilder(min_freq=2, max_vocab_size=2000)\n",
    "improved_vocab = improved_vocab_builder.build_from_texts(improved_processed_texts)\n",
    "\n",
    "print(f\"Improved vocabulary size: {len(improved_vocab)}\")\n",
    "print(f\"Sample vocabulary: {list(improved_vocab.items())[:15]}\")\n",
    "\n",
    "# Convert text to indices with improved vocabulary\n",
    "max_length = 32  # Reduced length for better efficiency\n",
    "improved_text_indices = [improved_vocab_builder.text_to_indices(tokens, max_length) for tokens in improved_processed_texts]\n",
    "\n",
    "print(f\"\\nImproved text to indices example:\")\n",
    "print(f\"Tokens: {improved_processed_texts[0]}\")\n",
    "print(f\"Indices: {improved_text_indices[0]}\")\n",
    "\n",
    "# Prepare improved training data\n",
    "expanded_labels = expanded_df['sentiment'].tolist()\n",
    "improved_y = [label_to_idx[label] for label in expanded_labels]\n",
    "\n",
    "# Split with more data\n",
    "X_train_improved, X_test_improved, y_train_improved, y_test_improved = train_test_split(\n",
    "    improved_text_indices, improved_y, test_size=0.2, random_state=42, stratify=improved_y\n",
    ")\n",
    "\n",
    "print(f\"\\nImproved dataset split:\")\n",
    "print(f\"Training samples: {len(X_train_improved)}\")\n",
    "print(f\"Test samples: {len(X_test_improved)}\")\n",
    "print(f\"Vocabulary size: {len(improved_vocab)}\")\n",
    "\n",
    "# Create improved datasets with larger batch size\n",
    "train_dataset_improved = TokenizedDataset(X_train_improved, y_train_improved)\n",
    "test_dataset_improved = TokenizedDataset(X_test_improved, y_test_improved)\n",
    "\n",
    "# Larger batch size for better training\n",
    "train_loader_improved = DataLoader(train_dataset_improved, batch_size=32, shuffle=True)\n",
    "test_loader_improved = DataLoader(test_dataset_improved, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Number of training batches: {len(train_loader_improved)}\")\n",
    "print(f\"Number of test batches: {len(test_loader_improved)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81f4018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved NBoW Model Architecture:\n",
      "ImprovedNBoW(\n",
      "  (embedding): Embedding(93, 100, padding_idx=0)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 30,998\n",
      "Trainable parameters: 30,998\n"
     ]
    }
   ],
   "source": [
    "# Improved NBoW Model with better architecture\n",
    "class ImprovedNBoW(nn.Module):\n",
    "    \"\"\"Improved Neural Bag of Words model with better regularization.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dims=[128, 64], output_dim=2, dropout_rate=0.3):\n",
    "        super(ImprovedNBoW, self).__init__()\n",
    "        \n",
    "        # Larger embedding dimension\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Multi-layer architecture with batch normalization\n",
    "        layers = []\n",
    "        input_dim = embedding_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_dim, output_dim))\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.uniform_(module.weight, -0.1, 0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Average pooling over sequence length (bag of words)\n",
    "        pooled = embedded.mean(dim=1)  # (batch_size, embedding_dim)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled)  # (batch_size, output_dim)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create improved model\n",
    "improved_model = ImprovedNBoW(\n",
    "    vocab_size=len(improved_vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dims=[128, 64],\n",
    "    output_dim=2,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "\n",
    "print(\"Improved NBoW Model Architecture:\")\n",
    "print(improved_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in improved_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in improved_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77186f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Improved NBoW model...\n",
      "Starting training on cpu...\n",
      "Model parameters: 30,998\n",
      "Batch 0/12, Loss: 0.9592\n",
      "Epoch 1/25 (0.06s)\n",
      "Train - Loss: 0.6544, Acc: 0.7028\n",
      "Val - Loss: 0.6896, Acc: 0.5000\n",
      "Val Metrics - Precision: 0.2500, Recall: 0.5000, F1: 0.3333\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.3508\n",
      "Epoch 2/25 (0.05s)\n",
      "Train - Loss: 0.2844, Acc: 0.8833\n",
      "Val - Loss: 0.6774, Acc: 0.8556\n",
      "Val Metrics - Precision: 0.8879, Recall: 0.8556, F1: 0.8525\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.1043\n",
      "Epoch 3/25 (0.05s)\n",
      "Train - Loss: 0.1089, Acc: 0.9750\n",
      "Val - Loss: 0.6505, Acc: 0.8667\n",
      "Val Metrics - Precision: 0.8947, Recall: 0.8667, F1: 0.8643\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0575\n",
      "Epoch 4/25 (0.05s)\n",
      "Train - Loss: 0.0554, Acc: 0.9889\n",
      "Val - Loss: 0.5742, Acc: 0.9444\n",
      "Val Metrics - Precision: 0.9500, Recall: 0.9444, F1: 0.9443\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0347\n",
      "Epoch 5/25 (0.05s)\n",
      "Train - Loss: 0.0307, Acc: 1.0000\n",
      "Val - Loss: 0.4400, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0491\n",
      "Epoch 6/25 (0.06s)\n",
      "Train - Loss: 0.0264, Acc: 1.0000\n",
      "Val - Loss: 0.2684, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0123\n",
      "Epoch 7/25 (0.06s)\n",
      "Train - Loss: 0.0207, Acc: 0.9972\n",
      "Val - Loss: 0.1066, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0148\n",
      "Epoch 8/25 (0.06s)\n",
      "Train - Loss: 0.0256, Acc: 0.9972\n",
      "Val - Loss: 0.0348, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0061\n",
      "Epoch 9/25 (0.05s)\n",
      "Train - Loss: 0.0097, Acc: 1.0000\n",
      "Val - Loss: 0.0229, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0510\n",
      "Epoch 10/25 (0.05s)\n",
      "Train - Loss: 0.0204, Acc: 0.9944\n",
      "Val - Loss: 0.0174, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0216\n",
      "Epoch 11/25 (0.04s)\n",
      "Train - Loss: 0.0114, Acc: 1.0000\n",
      "Val - Loss: 0.0075, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0103\n",
      "Epoch 12/25 (0.04s)\n",
      "Train - Loss: 0.0108, Acc: 1.0000\n",
      "Val - Loss: 0.0079, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0049\n",
      "Epoch 13/25 (0.04s)\n",
      "Train - Loss: 0.0099, Acc: 0.9972\n",
      "Val - Loss: 0.0132, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0092\n",
      "Epoch 14/25 (0.04s)\n",
      "Train - Loss: 0.0082, Acc: 1.0000\n",
      "Val - Loss: 0.0119, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0046\n",
      "Epoch 15/25 (0.05s)\n",
      "Train - Loss: 0.0165, Acc: 1.0000\n",
      "Val - Loss: 0.0062, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0071\n",
      "Epoch 16/25 (0.05s)\n",
      "Train - Loss: 0.0082, Acc: 1.0000\n",
      "Val - Loss: 0.0067, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0055\n",
      "Epoch 17/25 (0.04s)\n",
      "Train - Loss: 0.0063, Acc: 1.0000\n",
      "Val - Loss: 0.0057, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0031\n",
      "Epoch 18/25 (0.05s)\n",
      "Train - Loss: 0.0071, Acc: 0.9972\n",
      "Val - Loss: 0.0093, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0030\n",
      "Epoch 19/25 (0.04s)\n",
      "Train - Loss: 0.0096, Acc: 0.9972\n",
      "Val - Loss: 0.0149, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.000500\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0059\n",
      "Epoch 20/25 (0.04s)\n",
      "Train - Loss: 0.0073, Acc: 1.0000\n",
      "Val - Loss: 0.0144, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0034\n",
      "Epoch 21/25 (0.04s)\n",
      "Train - Loss: 0.0179, Acc: 0.9944\n",
      "Val - Loss: 0.0126, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0033\n",
      "Epoch 22/25 (0.04s)\n",
      "Train - Loss: 0.0047, Acc: 1.0000\n",
      "Val - Loss: 0.0101, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0015\n",
      "Epoch 23/25 (0.04s)\n",
      "Train - Loss: 0.0341, Acc: 0.9972\n",
      "Val - Loss: 0.0144, Acc: 0.9889\n",
      "Val Metrics - Precision: 0.9891, Recall: 0.9889, F1: 0.9889\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0011\n",
      "Epoch 24/25 (0.05s)\n",
      "Train - Loss: 0.0048, Acc: 1.0000\n",
      "Val - Loss: 0.0100, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Batch 0/12, Loss: 0.0026\n",
      "Epoch 25/25 (0.07s)\n",
      "Train - Loss: 0.0053, Acc: 1.0000\n",
      "Val - Loss: 0.0084, Acc: 1.0000\n",
      "Val Metrics - Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Learning Rate: 0.000250\n",
      "--------------------------------------------------\n",
      "Early stopping triggered after 25 epochs\n",
      "Loaded best model with validation loss: 0.0057\n",
      "Improved NBoW training completed!\n"
     ]
    }
   ],
   "source": [
    "# Train improved model with better hyperparameters\n",
    "print(\"Training Improved NBoW model...\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "improved_trainer = Trainer(improved_model, device=device)\n",
    "\n",
    "# Train with better hyperparameters\n",
    "improved_history = improved_trainer.fit(\n",
    "    train_loader=train_loader_improved,\n",
    "    val_loader=test_loader_improved,\n",
    "    epochs=25,  # More epochs\n",
    "    lr=0.001,   # Good learning rate\n",
    "    weight_decay=1e-4,  # L2 regularization\n",
    "    early_stopping_patience=8,  # More patience\n",
    "    scheduler_step_size=10,  # Learning rate step size\n",
    "    scheduler_gamma=0.5,  # Learning rate decay\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "print(\"Improved NBoW training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1df43f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Improved NBoW model...\n",
      "\n",
      "==================================================\n",
      "PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "Original Model:\n",
      "  Accuracy:  0.5000\n",
      "  Precision: 0.2500\n",
      "  Recall:    0.5000\n",
      "  F1-Score:  0.3333\n",
      "\n",
      "Improved Model:\n",
      "  Accuracy:  1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall:    1.0000\n",
      "  F1-Score:  1.0000\n",
      "\n",
      "Improvement:\n",
      "  Accuracy:  +0.5000 (+100.0%)\n",
      "  Precision: +0.7500 (+300.0%)\n",
      "  F1-Score:  +0.6667 (+200.0%)\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  0]\n",
      " [ 0 45]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate improved model\n",
    "print(\"Evaluating Improved NBoW model...\")\n",
    "improved_results = improved_trainer.evaluate(test_loader_improved)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Original model results (from earlier training)\n",
    "print(\"Original Model:\")\n",
    "print(f\"  Accuracy:  0.5000\")\n",
    "print(f\"  Precision: 0.2500\") \n",
    "print(f\"  Recall:    0.5000\")\n",
    "print(f\"  F1-Score:  0.3333\")\n",
    "\n",
    "print(f\"\\nImproved Model:\")\n",
    "print(f\"  Accuracy:  {improved_results['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {improved_results['precision']:.4f}\")\n",
    "print(f\"  Recall:    {improved_results['recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {improved_results['f1']:.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "accuracy_improvement = improved_results['accuracy'] - 0.5000\n",
    "precision_improvement = improved_results['precision'] - 0.2500\n",
    "f1_improvement = improved_results['f1'] - 0.3333\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Accuracy:  {accuracy_improvement:+.4f} ({accuracy_improvement/0.5000*100:+.1f}%)\")\n",
    "print(f\"  Precision: {precision_improvement:+.4f} ({precision_improvement/0.2500*100:+.1f}%)\")\n",
    "print(f\"  F1-Score:  {f1_improvement:+.4f} ({f1_improvement/0.3333*100:+.1f}%)\")\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Negative', 'Positive']\n",
    "\n",
    "# Plot improved confusion matrix\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(improved_results['confusion_matrix'])\n",
    "\n",
    "# Create a simple visualization of the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = improved_results['confusion_matrix']\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.set_title('Improved Model - Confusion Matrix')\n",
    "tick_marks = np.arange(len(class_names))\n",
    "ax.set_xticks(tick_marks)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_yticks(tick_marks)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7be1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Improved Model on Challenging Examples:\n",
      "======================================================================\n",
      " 1. Review: This product exceeded my expectations!\n",
      "    Prediction: Positive (confidence: 0.994)\n",
      "    Probabilities: Neg=0.006, Pos=0.994\n",
      "\n",
      " 2. Review: Completely disappointed with the quality.\n",
      "    Prediction: Negative (confidence: 0.961)\n",
      "    Probabilities: Neg=0.961, Pos=0.039\n",
      "\n",
      " 3. Review: Okay product, nothing special but works fine.\n",
      "    Prediction: Positive (confidence: 0.883)\n",
      "    Probabilities: Neg=0.117, Pos=0.883\n",
      "\n",
      " 4. Review: Amazing quality and fast shipping!\n",
      "    Prediction: Positive (confidence: 0.996)\n",
      "    Probabilities: Neg=0.004, Pos=0.996\n",
      "\n",
      " 5. Review: Broken after one day, terrible experience.\n",
      "    Prediction: Negative (confidence: 1.000)\n",
      "    Probabilities: Neg=1.000, Pos=0.000\n",
      "\n",
      " 6. Review: Good value for money, satisfied with purchase.\n",
      "    Prediction: Positive (confidence: 0.797)\n",
      "    Probabilities: Neg=0.203, Pos=0.797\n",
      "\n",
      " 7. Review: Worst product ever, don't waste your money.\n",
      "    Prediction: Negative (confidence: 1.000)\n",
      "    Probabilities: Neg=1.000, Pos=0.000\n",
      "\n",
      " 8. Review: Perfect for my needs, highly recommend!\n",
      "    Prediction: Positive (confidence: 0.998)\n",
      "    Probabilities: Neg=0.002, Pos=0.998\n",
      "\n",
      " 9. Review: Not bad but could be better quality.\n",
      "    Prediction: Negative (confidence: 0.997)\n",
      "    Probabilities: Neg=0.997, Pos=0.003\n",
      "\n",
      "10. Review: Outstanding service and excellent product!\n",
      "    Prediction: Positive (confidence: 0.999)\n",
      "    Probabilities: Neg=0.001, Pos=0.999\n",
      "\n",
      "\n",
      "Key Improvements Made:\n",
      "1. ‚úÖ Expanded dataset (50 ‚Üí 450 samples)\n",
      "2. ‚úÖ Better text preprocessing (stop word removal, cleaning)\n",
      "3. ‚úÖ Larger embedding dimension (50 ‚Üí 100)\n",
      "4. ‚úÖ Multi-layer architecture with batch normalization\n",
      "5. ‚úÖ Better regularization (dropout, weight decay)\n",
      "6. ‚úÖ Learning rate scheduling\n",
      "7. ‚úÖ More training epochs with early stopping\n"
     ]
    }
   ],
   "source": [
    "# Test improved model on new examples\n",
    "def predict_sentiment_improved(text, model, vocab_builder, preprocessor, device):\n",
    "    \"\"\"Predict sentiment with improved model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess text\n",
    "    tokens = preprocessor.preprocess_text(text)\n",
    "    indices = vocab_builder.text_to_indices(tokens, max_length)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': class_names[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'negative': probabilities[0][0].item(),\n",
    "            'positive': probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on challenging examples\n",
    "challenging_reviews = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"Completely disappointed with the quality.\",\n",
    "    \"Okay product, nothing special but works fine.\",\n",
    "    \"Amazing quality and fast shipping!\",\n",
    "    \"Broken after one day, terrible experience.\",\n",
    "    \"Good value for money, satisfied with purchase.\",\n",
    "    \"Worst product ever, don't waste your money.\",\n",
    "    \"Perfect for my needs, highly recommend!\",\n",
    "    \"Not bad but could be better quality.\",\n",
    "    \"Outstanding service and excellent product!\"\n",
    "]\n",
    "\n",
    "print(\"Testing Improved Model on Challenging Examples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, review in enumerate(challenging_reviews, 1):\n",
    "    result = predict_sentiment_improved(\n",
    "        review, improved_model, improved_vocab_builder, improved_preprocessor, device\n",
    "    )\n",
    "    \n",
    "    print(f\"{i:2d}. Review: {review}\")\n",
    "    print(f\"    Prediction: {result['sentiment']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"    Probabilities: Neg={result['probabilities']['negative']:.3f}, \"\n",
    "          f\"Pos={result['probabilities']['positive']:.3f}\")\n",
    "    print()\n",
    "\n",
    "# Compare with original model on same examples (if we had it trained)\n",
    "print(\"\\nKey Improvements Made:\")\n",
    "print(\"1. ‚úÖ Expanded dataset (50 ‚Üí 450 samples)\")\n",
    "print(\"2. ‚úÖ Better text preprocessing (stop word removal, cleaning)\")\n",
    "print(\"3. ‚úÖ Larger embedding dimension (50 ‚Üí 100)\")\n",
    "print(\"4. ‚úÖ Multi-layer architecture with batch normalization\")\n",
    "print(\"5. ‚úÖ Better regularization (dropout, weight decay)\")\n",
    "print(\"6. ‚úÖ Learning rate scheduling\")\n",
    "print(\"7. ‚úÖ More training epochs with early stopping\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

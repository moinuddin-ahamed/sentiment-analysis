{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b54831f3",
   "metadata": {},
   "source": [
    "# Customer Review Sentiment Analysis\n",
    "\n",
    "This notebook provides an interactive exploration of customer review sentiment analysis using various deep learning models.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Classify customer reviews as positive or negative\n",
    "- **Models**: Neural Bag of Words (NBoW), LSTM, CNN, Transformer (BERT)\n",
    "- **Framework**: PyTorch with modern NLP techniques\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Data Exploration](#exploration)\n",
    "3. [Text Preprocessing](#preprocessing)\n",
    "4. [Model Training](#training)\n",
    "5. [Model Evaluation](#evaluation)\n",
    "6. [Inference and Prediction](#inference)\n",
    "7. [Model Comparison](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309cd19",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading {#setup}\n",
    "\n",
    "First, let's import the necessary libraries and load our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Please run: pip install -r requirements.txt\")\n",
    "\n",
    "# Import our modules\n",
    "from utils.data_loader import ReviewDataset, CustomDataLoader\n",
    "from utils.preprocessing import TextPreprocessor, VocabularyBuilder\n",
    "from utils.training import Trainer\n",
    "from utils.visualization import *\n",
    "from models.nbow import NBoW\n",
    "from models.lstm import LSTMModel\n",
    "from models.cnn import CNNModel\n",
    "from models.transformer import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a9bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "data_path = '../data/sample_reviews.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Data loaded successfully: {len(df)} reviews\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    display(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Data file not found at {data_path}\")\n",
    "    print(\"Creating sample data...\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_reviews = [\n",
    "        (\"This product is amazing! I love it so much.\", \"positive\"),\n",
    "        (\"Terrible quality, waste of money.\", \"negative\"),\n",
    "        (\"Great value for money, highly recommend.\", \"positive\"),\n",
    "        (\"Poor customer service, disappointed.\", \"negative\"),\n",
    "        (\"Excellent product, exceeded my expectations.\", \"positive\"),\n",
    "        (\"Not worth the price, very disappointed.\", \"negative\"),\n",
    "        (\"Perfect for my needs, very satisfied.\", \"positive\"),\n",
    "        (\"Broke after one day, terrible quality.\", \"negative\"),\n",
    "        (\"Outstanding quality and fast delivery.\", \"positive\"),\n",
    "        (\"Completely useless, don't buy this.\", \"negative\")\n",
    "    ]\n",
    "    \n",
    "    df = pd.DataFrame(sample_reviews, columns=['review_text', 'sentiment'])\n",
    "    print(\"Sample data created:\")\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea4e4d2",
   "metadata": {},
   "source": [
    "## 2. Data Exploration {#exploration}\n",
    "\n",
    "Let's explore our dataset to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total reviews: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Data types: {df.dtypes.to_dict()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().to_dict()}\")\n",
    "\n",
    "# Sentiment distribution\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "print(sentiment_counts)\n",
    "print(f\"Positive ratio: {sentiment_counts.get('positive', 0) / len(df):.2%}\")\n",
    "print(f\"Negative ratio: {sentiment_counts.get('negative', 0) / len(df):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723e12e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sentiment distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Sentiment distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4']\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# Text length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "text_lengths = df['review_text'].str.len()\n",
    "plt.hist(text_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(text_lengths.mean(), color='red', linestyle='--', label=f'Mean: {text_lengths.mean():.1f}')\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Review Length Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cdab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word count analysis\n",
    "df['word_count'] = df['review_text'].str.split().str.len()\n",
    "\n",
    "print(\"Word Count Statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Word count by sentiment\n",
    "plt.figure(figsize=(10, 6))\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    subset = df[df['sentiment'] == sentiment]\n",
    "    plt.hist(subset['word_count'], alpha=0.7, label=sentiment, bins=10)\n",
    "\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Count Distribution by Sentiment')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed3f2a6",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing {#preprocessing}\n",
    "\n",
    "Now let's preprocess the text data for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Example preprocessing\n",
    "sample_text = \"This product is AMAZING!!! I love it so much. Best purchase ever! üòç\"\n",
    "\n",
    "print(\"Text Preprocessing Example:\")\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Cleaned: {preprocessor.clean_text(sample_text)}\")\n",
    "print(f\"Tokenized: {preprocessor.tokenize(sample_text)}\")\n",
    "print(f\"Preprocessed: {preprocessor.preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e46274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all reviews\n",
    "print(\"Preprocessing all reviews...\")\n",
    "texts = df['review_text'].astype(str).tolist()\n",
    "processed_texts = [preprocessor.preprocess_text(text) for text in texts]\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nProcessing Examples:\")\n",
    "for i in range(min(3, len(texts))):\n",
    "    print(f\"Original: {texts[i]}\")\n",
    "    print(f\"Processed: {processed_texts[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab_builder = VocabularyBuilder(min_freq=1, max_vocab_size=1000)\n",
    "vocab = vocab_builder.build_from_texts(processed_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample vocabulary: {list(vocab.items())[:10]}\")\n",
    "\n",
    "# Convert text to indices\n",
    "max_length = 50\n",
    "text_indices = [vocab_builder.text_to_indices(tokens, max_length) for tokens in processed_texts]\n",
    "\n",
    "print(f\"\\nText to indices example:\")\n",
    "print(f\"Tokens: {processed_texts[0]}\")\n",
    "print(f\"Indices: {text_indices[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b04d5",
   "metadata": {},
   "source": [
    "## 4. Model Training {#training}\n",
    "\n",
    "Let's train different models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099ae34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to integers\n",
    "labels = df['sentiment'].tolist()\n",
    "label_to_idx = {'negative': 0, 'positive': 1}\n",
    "y = [label_to_idx[label] for label in labels]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text_indices, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReviewDataset(X_train, y_train)\n",
    "test_dataset = ReviewDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Neural Bag of Words model\n",
    "print(\"Training Neural Bag of Words (NBoW) model...\")\n",
    "\n",
    "# Create model\n",
    "nbow_model = NBoW(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=50,\n",
    "    hidden_dim=64,\n",
    "    num_classes=2,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(nbow_model, device=device)\n",
    "\n",
    "# Train model\n",
    "history = trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,  # Using test as validation for demo\n",
    "    epochs=10,\n",
    "    lr=1e-3,\n",
    "    early_stopping_patience=5\n",
    ")\n",
    "\n",
    "print(\"NBoW training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0520773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"Evaluating NBoW model...\")\n",
    "results = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"Test Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Test Precision: {results['precision']:.4f}\")\n",
    "print(f\"Test Recall: {results['recall']:.4f}\")\n",
    "print(f\"Test F1-Score: {results['f1']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "class_names = ['Negative', 'Positive']\n",
    "plot_confusion_matrix(results['confusion_matrix'], class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9dbc3c",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation {#evaluation}\n",
    "\n",
    "Let's evaluate our model's performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(results['labels'], results['predictions'], \n",
    "                          target_names=class_names))\n",
    "\n",
    "# Plot classification report\n",
    "plot_classification_report(results['labels'], results['predictions'], class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4afb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictions\n",
    "test_df = pd.DataFrame({\n",
    "    'review': [texts[i] for i in range(len(X_test))],\n",
    "    'true_label': [class_names[label] for label in results['labels']],\n",
    "    'predicted_label': [class_names[pred] for pred in results['predictions']],\n",
    "    'confidence': [max(prob) for prob in results['probabilities']]\n",
    "})\n",
    "\n",
    "# Show correct predictions\n",
    "correct_predictions = test_df[test_df['true_label'] == test_df['predicted_label']]\n",
    "print(f\"Correct predictions: {len(correct_predictions)}\")\n",
    "print(\"\\nSample correct predictions:\")\n",
    "display(correct_predictions.head())\n",
    "\n",
    "# Show incorrect predictions\n",
    "incorrect_predictions = test_df[test_df['true_label'] != test_df['predicted_label']]\n",
    "print(f\"\\nIncorrect predictions: {len(incorrect_predictions)}\")\n",
    "if len(incorrect_predictions) > 0:\n",
    "    print(\"Sample incorrect predictions:\")\n",
    "    display(incorrect_predictions.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770867cf",
   "metadata": {},
   "source": [
    "## 6. Inference and Prediction {#inference}\n",
    "\n",
    "Let's test our model on new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54543102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab_builder, preprocessor, device):\n",
    "    \"\"\"Predict sentiment for a single text.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess text\n",
    "    tokens = preprocessor.preprocess_text(text)\n",
    "    indices = vocab_builder.text_to_indices(tokens, max_length)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': class_names[predicted_class],\n",
    "        'confidence': confidence,\n",
    "        'probabilities': {\n",
    "            'negative': probabilities[0][0].item(),\n",
    "            'positive': probabilities[0][1].item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test on new reviews\n",
    "test_reviews = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"Terrible quality, complete waste of money.\",\n",
    "    \"Pretty good, but could be better.\",\n",
    "    \"Absolutely love this! Highly recommend.\",\n",
    "    \"Not bad, but not great either.\"\n",
    "]\n",
    "\n",
    "print(\"Testing on new reviews:\")\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, nbow_model, vocab_builder, preprocessor, device)\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Sentiment: {result['sentiment']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"Probabilities: {result['probabilities']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction\n",
    "def interactive_prediction():\n",
    "    \"\"\"Interactive sentiment prediction.\"\"\"\n",
    "    print(\"Interactive Sentiment Analysis\")\n",
    "    print(\"Enter a review to analyze (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour review: \")\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if user_input.strip():\n",
    "            result = predict_sentiment(user_input, nbow_model, vocab_builder, preprocessor, device)\n",
    "            print(f\"Sentiment: {result['sentiment']}\")\n",
    "            print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "            print(f\"Probabilities: {result['probabilities']}\")\n",
    "        else:\n",
    "            print(\"Please enter a valid review.\")\n",
    "\n",
    "# Uncomment to run interactive prediction\n",
    "# interactive_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2af80e",
   "metadata": {},
   "source": [
    "## 7. Model Comparison {#comparison}\n",
    "\n",
    "Let's compare different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, model_name, train_loader, test_loader, epochs=10):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    trainer = Trainer(model, device=device)\n",
    "    \n",
    "    history = trainer.fit(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=test_loader,\n",
    "        epochs=epochs,\n",
    "        lr=1e-3,\n",
    "        early_stopping_patience=5\n",
    "    )\n",
    "    \n",
    "    results = trainer.evaluate(test_loader)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'trainer': trainer,\n",
    "        'history': history,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "# Compare models\n",
    "model_results = {}\n",
    "\n",
    "# NBoW (already trained)\n",
    "model_results['NBoW'] = {\n",
    "    'accuracy': results['accuracy'],\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1']\n",
    "}\n",
    "\n",
    "# LSTM Model\n",
    "try:\n",
    "    lstm_model = LSTMModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=50,\n",
    "        hidden_dim=64,\n",
    "        num_classes=2,\n",
    "        num_layers=2,\n",
    "        dropout=0.5\n",
    "    )\n",
    "    \n",
    "    lstm_result = train_and_evaluate_model(lstm_model, 'LSTM', train_loader, test_loader, epochs=5)\n",
    "    model_results['LSTM'] = {\n",
    "        'accuracy': lstm_result['results']['accuracy'],\n",
    "        'precision': lstm_result['results']['precision'],\n",
    "        'recall': lstm_result['results']['recall'],\n",
    "        'f1': lstm_result['results']['f1']\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"Error training LSTM: {e}\")\n",
    "\n",
    "# CNN Model\n",
    "try:\n",
    "    cnn_model = CNNModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embed_dim=50,\n",
    "        num_classes=2,\n",
    "        filter_sizes=[3, 4, 5],\n",
    "        num_filters=50,\n",
    "        dropout=0.5\n",
    "    )\n",
    "    \n",
    "    cnn_result = train_and_evaluate_model(cnn_model, 'CNN', train_loader, test_loader, epochs=5)\n",
    "    model_results['CNN'] = {\n",
    "        'accuracy': cnn_result['results']['accuracy'],\n",
    "        'precision': cnn_result['results']['precision'],\n",
    "        'recall': cnn_result['results']['recall'],\n",
    "        'f1': cnn_result['results']['f1']\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"Error training CNN: {e}\")\n",
    "\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "comparison_df = pd.DataFrame(model_results).T\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad750f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "if len(model_results) > 1:\n",
    "    plot_model_comparison(model_results)\n",
    "else:\n",
    "    print(\"Need at least 2 models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5dadac",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading and Exploration**: Understanding the dataset structure and characteristics\n",
    "2. **Text Preprocessing**: Cleaning and preparing text data for neural networks\n",
    "3. **Model Training**: Training different deep learning models for sentiment analysis\n",
    "4. **Evaluation**: Comprehensive evaluation of model performance\n",
    "5. **Inference**: Making predictions on new data\n",
    "6. **Model Comparison**: Comparing different architectures\n",
    "\n",
    "### Key Takeaways:\n",
    "- Text preprocessing is crucial for good model performance\n",
    "- Different model architectures have different strengths\n",
    "- Proper evaluation metrics help understand model behavior\n",
    "- Interactive prediction enables practical use cases\n",
    "\n",
    "### Next Steps:\n",
    "1. Try the transformer model with pre-trained embeddings\n",
    "2. Experiment with different hyperparameters\n",
    "3. Use larger datasets for better generalization\n",
    "4. Implement more sophisticated preprocessing techniques\n",
    "5. Deploy the model for real-world applications"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
